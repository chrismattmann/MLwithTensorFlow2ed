{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"../libs/basic_units/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import librosa\n",
    "import python_speech_features\n",
    "from basic_units import cm, inch\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.signal.windows import hann, hamming\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 2\n",
    "max_iterations = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_mfcc = 22\n",
    "n_mels = 40\n",
    "n_fft = 16384 \n",
    "hop_length = 2205\n",
    "fmin = 0\n",
    "fmax = None\n",
    "rate = 44000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(file):\n",
    "    file_contents = tf.io.read_file(file)\n",
    "    return file, file_contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = tf.io.match_filenames_once('../data/audio_dataset/*.wav')\n",
    "filename_ds = tf.data.Dataset.from_tensor_slices(filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename_contents_ds = filename_ds.map(read_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_next_chromagram(audio_file):\n",
    "    print('filename %s ' % (audio_file))\n",
    "    y, sr = librosa.load(audio_file, sr=rate)\n",
    "    winlen=n_fft / sr\n",
    "    winstep=hop_length/sr    \n",
    "    mfcc_speech = python_speech_features.mfcc(signal=y, samplerate=sr, winlen=winlen, winstep=winstep,\n",
    "                                          numcep=n_mfcc, nfilt=n_mels, nfft=n_fft, lowfreq=fmin, highfreq=fmax,\n",
    "                                          preemph=0.0, ceplifter=0, appendEnergy=False, winfunc=hamming)   \n",
    "    return mfcc_speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_feature_vector(chroma_data):\n",
    "    num_samples, num_features = np.shape(chroma_data)\n",
    "    print(\"Num features %d num samples %d \" % (num_features, num_samples))\n",
    "    freq_vals = tf.argmax(chroma_data)\n",
    "    hist, bins = np.histogram(freq_vals, bins=range(num_features + 1))\n",
    "    return hist.astype(float) / num_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset():\n",
    "    filename_contents_ds_enum = filename_contents_ds.enumerate()\n",
    "    xs = []\n",
    "    for file_obj in filename_contents_ds_enum.as_numpy_iterator():\n",
    "        chroma_data = get_next_chromagram(file_obj[1][0])\n",
    "        x = [extract_feature_vector(chroma_data)]\n",
    "        x = np.matrix(x)\n",
    "        if len(xs) == 0:\n",
    "            xs = x\n",
    "        else:\n",
    "            xs = np.vstack((xs, x))\n",
    "    \n",
    "    return xs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initial_cluster_centroids(X, k):\n",
    "    return X[0:k, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_cluster(X, centroids):\n",
    "    expanded_vectors = tf.expand_dims(X, 0) # 1, 5, 12\n",
    "    expanded_centroids = tf.expand_dims(centroids, 1) #2, 1, 12\n",
    "    distances = tf.reduce_sum(tf.square(tf.subtract(expanded_vectors, expanded_centroids)), 2) #2, 5\n",
    "    mins = tf.argmin(distances, 0)\n",
    "    return mins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recompute_centroids(X, Y):\n",
    "    sums = tf.math.unsorted_segment_sum(X, Y, k)\n",
    "    counts = tf.math.unsorted_segment_sum(tf.ones_like(X), Y, k)\n",
    "    return sums / counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filename b'../data/audio_dataset/cough_1.wav' \n",
      "Num features 22 num samples 16 \n",
      "filename b'../data/audio_dataset/cough_2.wav' \n",
      "Num features 22 num samples 25 \n",
      "filename b'../data/audio_dataset/scream_1.wav' \n",
      "Num features 22 num samples 19 \n",
      "filename b'../data/audio_dataset/scream_2.wav' \n",
      "Num features 22 num samples 43 \n",
      "filename b'../data/audio_dataset/scream_3.wav' \n",
      "Num features 22 num samples 61 \n",
      "tf.Tensor(\n",
      "[[0.         0.0625     0.125      0.1875     0.         0.\n",
      "  0.         0.125      0.0625     0.0625     0.125      0.1875\n",
      "  0.1875     0.         0.0625     0.1875     0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.01572627 0.01       0.03947368 0.03451251 0.01       0.00819672\n",
      "  0.01       0.00409836 0.         0.01315789 0.         0.03041415\n",
      "  0.0379437  0.03631579 0.03631579 0.03631579 0.04528764 0.02409836\n",
      "  0.07357204 0.01581395 0.02409836 0.02409836]], shape=(2, 22), dtype=float64)\n"
     ]
    }
   ],
   "source": [
    "X = get_dataset()\n",
    "print(X)\n",
    "print(X.shape)\n",
    "centroids = initial_cluster_centroids(X, k)\n",
    "i, converged = 0, False\n",
    "while not converged and i < max_iterations:\n",
    "    i += 1\n",
    "    Y = assign_cluster(X, centroids)\n",
    "    centroids = recompute_centroids(X, Y)\n",
    "print(centroids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
